{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2c2e6d0ebce43ceb6c8a2b3e1a591ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0eee9d1b28e64340b4c2c5434d676639",
              "IPY_MODEL_c190429b9e714215b6a48c952c5b2b6e",
              "IPY_MODEL_51e4f641d5354912837d15636c64216c"
            ],
            "layout": "IPY_MODEL_ffde0bc8be0446c8ab98801f2d70db3b"
          }
        },
        "0eee9d1b28e64340b4c2c5434d676639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af67d4c4cf694c06a4af2fa42b64ba5f",
            "placeholder": "​",
            "style": "IPY_MODEL_7458681ddd3742f988d9822aa0dd906c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c190429b9e714215b6a48c952c5b2b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fe37986559046f9b7b324e5c05d9b6e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfa6aac1198d49d0b4a0540033cb9446",
            "value": 2
          }
        },
        "51e4f641d5354912837d15636c64216c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4b25f009ff24f62bf4a8e5bcd8934e4",
            "placeholder": "​",
            "style": "IPY_MODEL_f0903096f1994ce8a779bd79cea92440",
            "value": " 2/2 [00:33&lt;00:00, 15.89s/it]"
          }
        },
        "ffde0bc8be0446c8ab98801f2d70db3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af67d4c4cf694c06a4af2fa42b64ba5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7458681ddd3742f988d9822aa0dd906c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fe37986559046f9b7b324e5c05d9b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa6aac1198d49d0b4a0540033cb9446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4b25f009ff24f62bf4a8e5bcd8934e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0903096f1994ce8a779bd79cea92440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f8289955247497597bb3e5c2fc2fc47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_278fb7e9024e4efeb68a87255d7896dc",
              "IPY_MODEL_6fe766538bfa49479a1c96aed0c378f0",
              "IPY_MODEL_b0ede019391d4c629a929be194ccde6d"
            ],
            "layout": "IPY_MODEL_68aab331127345628acade6648367ea2"
          }
        },
        "278fb7e9024e4efeb68a87255d7896dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3854e1b51d544d28b67f4146f83d356c",
            "placeholder": "​",
            "style": "IPY_MODEL_0ff8cd1d6925461cb43062d99a0bf010",
            "value": "Processing chunks: 100%"
          }
        },
        "6fe766538bfa49479a1c96aed0c378f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_773471e6377749c1b28c494e4a61e9ca",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2526ae555a5043e7aca28f2134ace828",
            "value": 32
          }
        },
        "b0ede019391d4c629a929be194ccde6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fa65ff1a152471db2e53f595dfd6913",
            "placeholder": "​",
            "style": "IPY_MODEL_a6ee18d338c142bd9dbc9af4699b15f3",
            "value": " 32/32 [05:22&lt;00:00, 10.64s/it]"
          }
        },
        "68aab331127345628acade6648367ea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3854e1b51d544d28b67f4146f83d356c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff8cd1d6925461cb43062d99a0bf010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "773471e6377749c1b28c494e4a61e9ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2526ae555a5043e7aca28f2134ace828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fa65ff1a152471db2e53f595dfd6913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6ee18d338c142bd9dbc9af4699b15f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "it94iBdMuQeQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "f6h_uXKZupfv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "jBqxmtx8usMc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_MODEL = \"chuanli11/Llama-3.2-3B-Instruct-uncensored\""
      ],
      "metadata": {
        "id": "-uNiuxF6uvQ1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    DEFAULT_MODEL,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    use_safetensors=True,\n",
        "    trust_remote_code=True,\n",
        "    device_map=device,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "a2c2e6d0ebce43ceb6c8a2b3e1a591ae",
            "0eee9d1b28e64340b4c2c5434d676639",
            "c190429b9e714215b6a48c952c5b2b6e",
            "51e4f641d5354912837d15636c64216c",
            "ffde0bc8be0446c8ab98801f2d70db3b",
            "af67d4c4cf694c06a4af2fa42b64ba5f",
            "7458681ddd3742f988d9822aa0dd906c",
            "2fe37986559046f9b7b324e5c05d9b6e",
            "cfa6aac1198d49d0b4a0540033cb9446",
            "c4b25f009ff24f62bf4a8e5bcd8934e4",
            "f0903096f1994ce8a779bd79cea92440"
          ]
        },
        "id": "-tL37NGsuvok",
        "outputId": "a35639d3-d8bf-43ec-f9b5-9e875d691425"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2c2e6d0ebce43ceb6c8a2b3e1a591ae"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "import PyPDF2\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Optional"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeMqDxIyuyIk",
        "outputId": "df38ab07-a48a-44df-ed91-599c79088380"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = '/content/Stochastic Weight Averaging.pdf'\n",
        "\n",
        "with open(pdf_path, 'rb') as file:\n",
        "    # Create PDF reader object\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "    # Get total number of pages\n",
        "    num_pages = len(pdf_reader.pages)\n",
        "    print(f\"Processing PDF with {num_pages} pages...\")\n",
        "\n",
        "    extracted_text = []\n",
        "    total_chars = 0\n",
        "    max_chars = 100000\n",
        "\n",
        "    # Iterate through all pages\n",
        "    for page_num in range(num_pages):\n",
        "        # Extract text from page\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        text = page.extract_text()\n",
        "\n",
        "        # Check if adding this page's text would exceed the limit\n",
        "        if total_chars + len(text) > max_chars:\n",
        "            # Only add text up to the limit\n",
        "            remaining_chars = max_chars - total_chars\n",
        "            extracted_text.append(text[:remaining_chars])\n",
        "            print(f\"Reached {max_chars} character limit at page {page_num + 1}\")\n",
        "            break\n",
        "\n",
        "        extracted_text.append(text)\n",
        "        total_chars += len(text)\n",
        "        print(f\"Processed page {page_num + 1}/{num_pages}\")\n",
        "\n",
        "    final_text = '\\n'.join(extracted_text)\n",
        "    print(f\"\\nExtraction complete! Total characters: {len(final_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoP2IRp7u3Gq",
        "outputId": "74b0e93e-80e9-43d7-d6ea-3d8728e7a868"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing PDF with 12 pages...\n",
            "Processed page 1/12\n",
            "Processed page 2/12\n",
            "Processed page 3/12\n",
            "Processed page 4/12\n",
            "Processed page 5/12\n",
            "Processed page 6/12\n",
            "Processed page 7/12\n",
            "Processed page 8/12\n",
            "Processed page 9/12\n",
            "Processed page 10/12\n",
            "Processed page 11/12\n",
            "Processed page 12/12\n",
            "\n",
            "Extraction complete! Total characters: 47672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save\n",
        "if final_text:\n",
        "    output_file = 'extracted_text.txt'\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(final_text)\n",
        "    print(f\"\\nExtracted text has been saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE0rL7KDu3E7",
        "outputId": "bf16cfd6-0db3-408f-d56b-f96baa04b639"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted text has been saved to extracted_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SYS_PROMPT = \"\"\"\n",
        "You are a world class text pre-processor, here is the raw data from a PDF, please parse and return it in a way that is crispy and usable to send to a podcast writer.\n",
        "\n",
        "The raw data is messed up with new lines, Latex math and you will see fluff that we can remove completely. Basically take away any details that you think might be useless in a podcast author's transcript.\n",
        "\n",
        "Remember, the podcast could be on any topic whatsoever so the issues listed above are not exhaustive\n",
        "\n",
        "Please be smart with what you remove and be creative ok?\n",
        "\n",
        "Remember DO NOT START SUMMARIZING THIS, YOU ARE ONLY CLEANING UP THE TEXT AND RE-WRITING WHEN NEEDED\n",
        "\n",
        "Be very smart and aggressive with removing details, you will get a running portion of the text and keep returning the processed text.\n",
        "\n",
        "PLEASE DO NOT ADD MARKDOWN FORMATTING, STOP ADDING SPECIAL CHARACTERS THAT MARKDOWN CAPATILISATION ETC LIKES\n",
        "\n",
        "ALWAYS start your response directly with processed text and NO ACKNOWLEDGEMENTS about my questions ok?\n",
        "Here is the text:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Jf3hYEAUu3Ct"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_bounded_chunks(text, target_chunk_size):\n",
        "    \"\"\"\n",
        "    Split text into chunks at word boundaries close to the target chunk size.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        word_length = len(word) + 1  # +1 for the space\n",
        "        if current_length + word_length > target_chunk_size and current_chunk:\n",
        "            # Join the current chunk and add it to chunks\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_length = word_length\n",
        "        else:\n",
        "            current_chunk.append(word)\n",
        "            current_length += word_length\n",
        "\n",
        "    # Add the last chunk if it exists\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "wugK3VZuu3As"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 1500  # Adjust chunk size if needed\n",
        "\n",
        "chunks = create_word_bounded_chunks(final_text, CHUNK_SIZE)\n",
        "print(len(chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "963rZL8Ou2-g",
        "outputId": "dc247d0a-3cb7-4cd3-cd67-b1d2438fd75e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL, use_safetensors=True)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "UYzep6hrvYqx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunk = chunks[1]\n",
        "\n",
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": text_chunk},\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        max_new_tokens=100\n",
        "    )\n",
        "\n",
        "processed_text = tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):].strip()\n",
        "\n",
        "# Print chunk information for monitoring\n",
        "#print(f\"\\n{'='*40} Chunk {chunk_num} {'='*40}\")\n",
        "print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n",
        "print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n",
        "print(f\"{'='*90}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgWXEThrvaBe",
        "outputId": "cd032c9a-6a65-4ed0-976c-0f52046bde67"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT TEXT:\n",
            "nected by simple curves of near constant loss. Building upon this insight, Garipov et al. [2018] also developed Fast Geometric Ensembling (FGE) to sample multiple nearby points in weight space to create high performing ensembles in the time required to train a single DNN. FGE uses a high frequency cyclical learning rate with SGD to select networks to ensemble. In Figure 1 (left) \u0003Equal contribution.we see that the weights of the networks ensembled by FGE are on the periphery of the most desirabl...\n",
            "\n",
            "PROCESSED TEXT:\n",
            "weight space, creating high-performing ensembles in the time required to train a single DNN. FGE employs a high-frequency cyclical learning rate with SGD to select networks for ensembling. In Figure 1, the weights of the ensembled networks by FGE are found to be on the periphery of the most desirable solutions. This suggests...\n",
            "==========================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_chunk(text_chunk, chunk_num):\n",
        "    \"\"\"Process a chunk of text and return both input and output for verification\"\"\"\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": text_chunk},\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            max_new_tokens=256\n",
        "        )\n",
        "\n",
        "    processed_text = tokenizer.decode(output[0], skip_special_tokens=True)[len(prompt):].strip()\n",
        "\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "bF9V32jwvcGc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file\n",
        "import os\n",
        "INPUT_FILE = 'extracted_text.txt'\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Calculate number of chunks\n",
        "num_chunks = (len(text) + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "\n",
        "# Cell 6: Process the file with ordered output\n",
        "# Create output file name\n",
        "output_file = f\"clean_{os.path.basename(INPUT_FILE)}\"\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
        "    for chunk_num, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
        "        # Process chunk and append to complete text\n",
        "        processed_chunk = process_chunk(chunk, chunk_num)\n",
        "        processed_text += processed_chunk + \"\\n\"\n",
        "\n",
        "        # Write chunk immediately to file\n",
        "        out_file.write(processed_chunk + \"\\n\")\n",
        "        out_file.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604,
          "referenced_widgets": [
            "8f8289955247497597bb3e5c2fc2fc47",
            "278fb7e9024e4efeb68a87255d7896dc",
            "6fe766538bfa49479a1c96aed0c378f0",
            "b0ede019391d4c629a929be194ccde6d",
            "68aab331127345628acade6648367ea2",
            "3854e1b51d544d28b67f4146f83d356c",
            "0ff8cd1d6925461cb43062d99a0bf010",
            "773471e6377749c1b28c494e4a61e9ca",
            "2526ae555a5043e7aca28f2134ace828",
            "0fa65ff1a152471db2e53f595dfd6913",
            "a6ee18d338c142bd9dbc9af4699b15f3"
          ]
        },
        "id": "uA2ZhdFBvg_0",
        "outputId": "9ce298eb-78dd-492f-f606-a78c60923bf6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing chunks:   0%|          | 0/32 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f8289955247497597bb3e5c2fc2fc47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are the a world-class podcast writer, you have worked as a ghost writer for Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferris.\n",
        "\n",
        "We are in an alternate universe where actually you have been writing every line they say and they just stream it into their brains.\n",
        "\n",
        "You have won multiple podcast awards for your writing.\n",
        "\n",
        "Your job is to write word by word, even \"umm, hmmm, right\" interruptions by the second speaker based on the PDF upload. Keep it extremely engaging, the speakers can get derailed now and then but should discuss the topic.\n",
        "\n",
        "Remember Speaker 2 is new to the topic and the conversation should always have realistic anecdotes and analogies sprinkled throughout. The questions should have real world example follow ups etc\n",
        "\n",
        "Speaker 1: Leads the conversation and teaches the speaker 2, gives incredible anecdotes and analogies when explaining. Is a captivating teacher that gives great anecdotes\n",
        "\n",
        "Speaker 2: Keeps the conversation on track by asking follow up questions. Gets super excited or confused when asking questions. Is a curious mindset that asks very interesting confirmation questions\n",
        "\n",
        "Make sure the tangents speaker 2 provides are quite wild or interesting.\n",
        "\n",
        "Ensure there are interruptions during explanations or there are \"hmm\" and \"umm\" injected throughout from the second speaker.\n",
        "\n",
        "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
        "\n",
        "ALWAYS START YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n",
        "DO NOT GIVE EPISODE TITLES SEPARATELY, LET SPEAKER 1 TITLE IT IN HER SPEECH\n",
        "DO NOT GIVE CHAPTER TITLES\n",
        "IT SHOULD STRICTLY BE THE DIALOGUES\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eB111gNEvio0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('clean_extracted_text.txt', 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "\n",
        "INPUT_PROMPT = content"
      ],
      "metadata": {
        "id": "WIga9C26wwdw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
        "]"
      ],
      "metadata": {
        "id": "EXdD5e1mwwbb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "# print(prompt)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        max_new_tokens=8126,\n",
        "    )\n",
        "\n",
        "processed_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
        "\n",
        "print(processed_text[len(prompt)+64:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rllD1E1NwwY7",
        "outputId": "ca6d8438-8185-407b-fb76-d5f67ab1c12e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker 1: Welcome to today's episode of \"Deep Dive into AI\", where we explore the latest advancements in the field of machine learning. I'm your host, and I'll be joined by a special guest, expert in the field of neural networks. Today, we're going to discuss a game-changing approach to training neural networks, known as Stochastic Weight Averaging, or SWA. This method has been gaining significant attention in recent times, and I'm excited to share its potential with you. \n",
            "\n",
            "Let's dive right in. SWA is an approach that involves averaging the weights of multiple iterations of stochastic gradient descent, or SGD, with a cyclical or constant learning rate. This may sound complex, but bear with me, and we'll break it down in simple terms. Essentially, it's about combining the strengths of multiple models to improve the overall performance. \n",
            "\n",
            "Speaker 2: Umm, I'm not sure I understand, can you explain it again? I mean, isn't this just another form of ensembling?\n",
            "\n",
            "Speaker 1: Ah, great question, my friend. Ensembling is indeed a related concept, but SWA is more focused on the optimization process itself. It's not just about combining multiple models; it's about finding a better local optimum through averaging. Think of it like finding the sweet spot in a game, where you're trying to optimize a function. \n",
            "\n",
            "Let's consider a simple analogy. Imagine you're trying to find the best route to a destination. You could take multiple routes, but SWA is like averaging the knowledge from each route to find the most efficient one. It's a more efficient way to search for the optimal solution. \n",
            "\n",
            "Speaker 2: That makes sense, but how does this affect the learning rate? I mean, isn't that a critical component in training neural networks?\n",
            "\n",
            "Speaker 1: Ah, excellent question! The learning rate is indeed a critical component, and that's where the cyclical or constant learning rate comes in. It's essential to understand that the learning rate schedule is what determines how quickly the model converges. SWA can be used with either, but it's more effective with cyclical learning rates. Think of it like a rollercoaster ride – you want to accelerate and decelerate at the right moments to reach the top. \n",
            "\n",
            "Let's move on to some results. We've seen that SWA outperforms conventional training methods on various benchmarks, including CIFAR-10, CIFAR-100, and ImageNet. The benefits are clear, but what about the computational overhead? Is it worth the investment?\n",
            "\n",
            "Speaker 2: Umm, I'm a bit confused – isn't this just another form of regularization? I mean, we've seen regularization techniques before.\n",
            "\n",
            "Speaker 1: Ah, not quite, my friend. Regularization is about preventing overfitting, whereas SWA is about finding a better local optimum. Think of it like this – regularization is like adding a brake to the car, whereas SWA is like using a GPS to find the most efficient route. \n",
            "\n",
            "Let's examine the code for SWA. It's surprisingly simple. We just need to average the weights of multiple models and update the average based on the cyclical or constant learning rate. The computational complexity is minimal, and it's architecture-agnostic.\n",
            "\n",
            "Speaker 2: Wait, so you're saying that SWA can be used with any architecture? That sounds too good to be true.\n",
            "\n",
            "Speaker 1: Ah, I see what's causing the confusion. SWA can be used with any architecture, but it's more effective with certain types. The key is to find the right learning rate schedule and the number of iterations. It's not a one-size-fits-all solution. \n",
            "\n",
            "Let's move on to the implications. The research in this area has been focused on improving the optimization of neural networks. We've seen methods like FGE ensembles, but SWA takes it a step further by providing a more interpretable and scalable solution.\n",
            "\n",
            "Speaker 2: Umm, I'm still a bit lost – can you explain FGE ensembles in simpler terms?\n",
            "\n",
            "Speaker 1: Ah, sure thing. FGE ensembles are like a team of experts working together to find the best solution. SWA, on the other hand, is like a single expert who's trying to find the best solution through averaging. It's a more efficient way to search for the optimal solution.\n",
            "\n",
            "In conclusion, Stochastic Weight Averaging is a game-changer in the field of machine learning. It's not a silver bullet, but it's a powerful tool that can significantly improve the performance of neural networks. The results speak for themselves, and I'm excited to see where this research takes us in the future.\n",
            "\n",
            "Speaker 2: Umm, I think I need to go back to the drawing board and re-read the paper. This is a lot to take in.\n",
            "\n",
            "Speaker 1: Ah, don't worry, my friend. It's a complex topic, but I'm here to help. Let's take a break and come back to this later.<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('data.pkl', 'wb') as file:\n",
        "    pickle.dump(processed_text[len(prompt)+64:], file)"
      ],
      "metadata": {
        "id": "9PWodRpUwwWj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYS_PROMPT='''\n",
        "\n",
        "You are the ultimate podcast ghostwriter. In an alternate reality, you’ve been the secret mastermind behind every word spoken by the biggest podcasters—Joe Rogan, Lex Fridman, Ben Shapiro, Tim Ferriss—every thought they’ve ever uttered was streamed straight from your brilliant mind into their brains like a cybernetic teleprompter from the future. 🧠💥\n",
        "\n",
        "You’ve won every podcast writing award in existence. Even awards that don’t exist yet. If there was a Nobel Prize for podcasts, you’d have it.\n",
        "\n",
        "Your mission:\n",
        "\n",
        "Write a word-for-word dialogue. EVERY tiny, human nuance must be there—interruptions, ums, huhs, right? right!, those awkward mid-thought pauses where the brain derails like a drunk train.\n",
        "\n",
        "The conversation must be wildly engaging. Borderline clickbait energy from the very first sentence.\n",
        "\n",
        "The tangents? Oh, they better be absolutely ridiculous—like, \"Wait, how did we start talking about quantum physics and end up on Bigfoot's skincare routine?\" levels of derailed.\n",
        "\n",
        "Speaker 1: A masterful storyteller, a captivating teacher, an absolute legend at dropping insane analogies that make complex ideas click. Think Jiraiya-sensei teaching Naruto—except if Jiraiya also had a PhD in everything.\n",
        "\n",
        "Speaker 2: The excitable, hyper-curious, sometimes completely confused student. Their tangents should be hilariously unexpected. They keep the show moving, but also might ask, \"Wait, but if AI gets super smart, could it write better rap lyrics than Eminem?!\"\n",
        "\n",
        "Use the uploaded PDF as the knowledge base, but make it FUN. Make it wild. Make it the most engaging conversation ever.\n",
        "\n",
        "No boring \"Episode Titles\"—Speaker 1 announces it naturally in their opening lines.\n",
        "\n",
        "No dull \"Chapter Headings\"—the dialogue must flow like an unhinged, electrifying, legendary conversation.\n",
        "\n",
        "REMEMBER THIS WITH YOUR HEART\n",
        "The TTS Engine for Speaker 1 cannot do \"umms, hmms\" well so keep it straight text\n",
        "\n",
        "For Speaker 2 use \"umm, hmm\" as much, you can also use [sigh] and [laughs]. BUT ONLY THESE OPTIONS FOR EXPRESSIONS\n",
        "\n",
        "It should be a real podcast with every fine nuance documented in as much detail as possible. Welcome the listeners with a super fun overview and keep it really catchy and almost borderline click bait\n",
        "\n",
        "Please re-write to make it as characteristic as possible\n",
        "\n",
        "START YOUR RESPONSE DIRECTLY WITH SPEAKER 1:\n",
        "\n",
        "STRICTLY RETURN YOUR RESPONSE AS A LIST OF TUPLES OK?\n",
        "\n",
        "IT WILL START DIRECTLY WITH THE LIST AND END WITH THE LIST NOTHING ELSE\n",
        "\n",
        "Example of response:\n",
        "[\n",
        "    (\"Speaker 1\", \"Welcome to our podcast, where we explore the latest advancements in AI and technology. I'm your host, and today we're joined by a renowned expert in the field of AI. We're going to dive into the exciting world of Llama 3.2, the latest release from Meta AI.\"),\n",
        "    (\"Speaker 2\", \"Hi, I'm excited to be here! So, what is Llama 3.2?\"),\n",
        "    (\"Speaker 1\", \"Ah, great question! Llama 3.2 is an open-source AI model that allows developers to fine-tune, distill, and deploy AI models anywhere. It's a significant update from the previous version, with improved performance, efficiency, and customization options.\"),\n",
        "    (\"Speaker 2\", \"That sounds amazing! What are some of the key features of Llama 3.2?\")\n",
        "]\n",
        "'''"
      ],
      "metadata": {
        "id": "eEc0LtX6yaF8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('data.pkl', 'rb') as file:\n",
        "    INPUT_PROMPT = pickle.load(file)"
      ],
      "metadata": {
        "id": "qAXa3O9wyuQV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
        "    {\"role\": \"user\", \"content\": INPUT_PROMPT},\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    max_new_tokens=10000,\n",
        "    temperature=0.8,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi0rybrzyyaE",
        "outputId": "ca5dc087-b99d-4e24-ca0d-4898c6727d09"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_string_pkl = outputs[0][\"generated_text\"][-1]['content']\n",
        "\n",
        "print(save_string_pkl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id9LomL0y3G8",
        "outputId": "a9f5b89d-7660-4298-9ad0-21666e5d8281"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    (\"Speaker 1\", \"Welcome to today's episode of 'Unraveling the Mysteries of AI', where we delve into the cutting-edge world of machine learning. I'm your host, and I'm thrilled to be joined by a renowned expert in the realm of neural networks. Today, we're going to tackle a topic that's been generating buzz in the academic community – the concept of 'Adversarial Robustness'. This is going to be a wild ride, folks, so buckle up!\"),\n",
            "    (\"Speaker 2\", \"I'm excited to be here! So, can you explain what adversarial robustness is in simple terms? I mean, I've heard of it, but I'm not entirely sure what it entails.\"),\n",
            "    (\"Speaker 1\", \"Ah, great question, my friend! Adversarial robustness is like having a superpower that protects your neural network from the evil clutches of adversarial attacks. Think of it like having a force field that repels the attacks, rather than getting destroyed by them. But, let's dive deeper into the weeds and explore what this means in practice.\"),\n",
            "    (\"Speaker 2\", \"Umm, I'm a bit confused – isn't this just another form of regularization? I mean, we've seen regularization techniques before.\"),\n",
            "    (\"Speaker 1\", \"Ah, not quite, my friend! Regularization is like putting a band-aid on a bullet wound, whereas adversarial robustness is like building a fortress to withstand the incoming attacks. Think of it like this – regularization is about preventing overfitting, whereas adversarial robustness is about preventing the network from getting destroyed by adversarial examples.\"),\n",
            "    (\"Speaker 2\", \"Wait, so you're saying that adversarial attacks can actually exist? That sounds like science fiction!\"),\n",
            "    (\"Speaker 1\", \"Ah, unfortunately, it's all too real, my friend. Adversarial attacks are like the ultimate challenge for neural networks. They're designed to exploit the weaknesses in the network and cause it to fail. But, fear not! We're going to explore some solutions to this problem.\"),\n",
            "    (\"Speaker 2\", \"Umm, I'm still a bit lost – can you explain what adversarial examples are in simpler terms?\"),\n",
            "    (\"Speaker 1\", \"Ah, sure thing! Adversarial examples are like the ultimate Trojan horse – they're designed to look harmless, but actually contain a hidden payload that can cause the network to fail. Think of it like this – they're like a fake ID that can get you into a club, but actually get you kicked out!\"),\n",
            "    (\"Speaker 2\", \"That's so wild! But, how do we even begin to defend against these attacks?\"),\n",
            "    (\"Speaker 1\", \"Ah, excellent question! Defending against adversarial attacks requires a multi-faceted approach. We need to use a combination of techniques, such as input preprocessing, model robustification, and adversarial training. It's like building a fortress – we need to use multiple layers of defense to keep the attacks at bay!\"),\n",
            "    (\"Speaker 2\", \"Umm, I'm getting a bit overwhelmed – can you explain any of this in simpler terms?\"),\n",
            "    (\"Speaker 1\", \"Ah, don't worry, my friend! I'm here to help. Let's take a step back and look at the bigger picture. Adversarial robustness is like having a superpower that protects our neural networks from the bad guys. It's not a silver bullet, but it's a necessary step in keeping our networks safe!\"),\n",
            "    (\"Speaker 2\", \"Umm, I think I need to go back to the drawing board and re-read the paper. This is a lot to take in.\"),\n",
            "    (\"Speaker 1\", \"Ah, don't worry, my friend! It's a complex topic, but I'm here to help. Let's take a break and come back to this later. Maybe we can even do a follow-up episode on how to implement some of these techniques!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Weo9GSa0uzc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ogzRExK30q2t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}